# 简介

Web服务器收到的HTTP请求一般都来自Web浏览器，有时也可能来自Web爬虫等软件。

## Web爬虫

：一些程序或脚本，会自动向Web服务器发出HTTP请求报文，并从HTTP响应报文中解析出重要的数据。
- 很多种编程语言都可以开发Web爬虫，其中，使用Python开发爬虫尤其方便。
- Web爬虫可用于代替人工收集Web网站的数据，效率很高。但是Web爬虫也可能被用于非法获取Web网站的数据，而且频繁的爬取会消耗Web网站的大量带宽，因此很多网站会采取反制爬虫的措施。

## robots.txt

：可以放在网站根目录下的一个文本文件。
- 它代表爬虫协议、机器人协议，用于告诉搜索引擎、爬虫允许访问该网站的哪些资源、不允许访问哪些资源。不过只是一种声明，不具有强制的约束力。
- 当搜索引擎、爬虫访问一个网站时，应该先检查该网站的根目录下是否存在robots.txt文件。如果存在，则遵守其要求，限制访问范围；如果不存在，则默认可以访问该网站的所有资源。
- 内容示例：
    ```
    User-agent: *           # 通配符*指代所有robot
    Disallow: /             # 禁止访问根目录下的所有文件
    Allow: /tmp             # 允许访问/tmp目录
    Allow: .jpg$            # 允许访问所有.jpg格式的图片
    ```

## sitemap.xml

：可以放在网站根目录下的一个文本文件。
- 又称为网站地图，以XML格式存储了该网站的所有链接，方便被搜索引擎爬取、收录。

## SEO

：搜索引擎优化（Search Engine Optimization）
- SEO有多种实现方式，目的都是提高网站在搜索引擎的排名的，从而提高网站的访问量。
