# Web 爬虫

Web 服务器收到的 HTTP 请求一般都来自真人用户操纵的 Web 浏览器，有时也可能来自 Web 爬虫等软件。

## 爬虫

：一些程序或脚本，会自动向 Web 服务器发出 HTTP 请求报文，并从 HTTP 响应报文中解析出重要的数据。
- 很多种编程语言都可以开发 Web 爬虫，其中，使用 Python 开发爬虫尤其方便。
- Web 爬虫可用于代替人工收集 Web 网站的数据，效率很高。但是 Web 爬虫也可能被用于非法获取 Web 网站的数据，而且频繁的爬取会消耗 Web 网站的大量带宽，因此很多网站会采取反制爬虫的措施。

## robots.txt

：可以放在网站根目录下的一个文本文件。
- 它代表爬虫协议、机器人协议，用于告诉搜索引擎、爬虫允许访问该网站的哪些资源、不允许访问哪些资源。不过只是一种声明，不具有强制的约束力。
- 当搜索引擎、爬虫访问一个网站时，应该先检查该网站的根目录下是否存在 robots.txt 文件。如果存在，则遵守其要求，限制访问范围；如果不存在，则默认可以访问该网站的所有资源。
- 内容示例：
    ```
    User-agent: *           # 通配符*指代所有 robot
    Disallow: /             # 禁止访问根目录下的所有文件
    Allow: /tmp             # 允许访问/tmp 目录
    Allow: .jpg$            # 允许访问所有.jpg 格式的图片
    ```

## sitemap.xml

：可以放在网站根目录下的一个文本文件。
- 又称为网站地图，以 XML 格式存储了该网站的所有链接，方便被搜索引擎爬取、收录。

## SEO

：搜索引擎优化（Search Engine Optimization）
- SEO 有多种实现方式，目的都是提高网站在搜索引擎的排名的，从而提高网站的访问量。
