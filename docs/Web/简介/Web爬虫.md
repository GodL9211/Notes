# Web 爬虫

Web 服务器收到的 HTTP 请求一般都来自真人用户操纵的 Web 浏览器，有时也可能来自 Web 爬虫等软件。

## 爬虫

：一些程序，会自动向 Web 网站发出 HTTP 请求，并从 HTTP 响应报文中提取数据，比人工收集数据的效率高很多。
- 很多种编程语言都可用于开发 Web 爬虫。其中，使用 Python 开发爬虫尤其方便。
- Web 爬虫可能被用于非法获取网站的私密数据，还可能消耗网站的大量带宽、增加负载。因此，很多网站会采取一些反制爬虫的措施，比如加验证码。

## 相关概念

### robots.txt

：放在网站根目录下的一个文本文件，用于声明该网站的哪些资源允许被搜索引擎、爬虫访问。
- 它只是一种声明，不具有强制的约束力。
  - 当搜索引擎、爬虫访问一个网站时，应该先检查该网站的根目录下是否存在 robots.txt 文件。如果存在，则遵守其要求，限制访问范围；如果不存在，则默认可以访问该网站的所有资源。
- 内容示例：
  ```sh
  User-agent: *           # 声明该规则作用于哪些 robot ，可以使用通配符
  Disallow: /js/*         # 禁止访问指定路径的文件
  Allow: /img/            # 允许访问指定路径的文件

  Sitemap: https://test.com/sitemap.xml
  ```

### sitemap.xml

：放在网站根目录下的一个文本文件，以 XML 格式存储了该网站的所有链接，作为网站地图，方便被搜索引擎爬取、收录。
- 也可以放在网站其它路径，然后在 robots.txt 中声明。

### SEO

：搜索引擎优化（Search Engine Optimization），指通过多种方式提高网站在搜索引擎的排名，从而提高网站的曝光量、访问量。
