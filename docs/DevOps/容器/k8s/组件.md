# 组件

- k8s 包含多个组件进程，通常部署在多个主机上，组成分布式集群。
  - 每个主机称为节点（Node），分为两种类型：
    - 控制平面节点（Control Plane Node）：又称为主节点（master node），负责控制整个集群、管理所有节点。
    - 工作节点（Worker Node）：负责部署 Pod 。
  - 部署 k8s 组件时，可以直接运行二进制文件，也可以容器化部署。

- 主节点一般运行以下进程：
  - kube-apiserver
  - kube-controller-manager
  - kube-scheduler
  - etcd

- 所有节点都运行以下进程：
  - kubelet
  - kube-proxy

## kube-apiserver

- 功能：提供 Restful API ，供用户、k8s 组件访问，从而管理 k8s 集群。
  - 例如，用户可使用 kubectl 命令，作为客户端与 apiserver 交互，从而管理 k8s 。
- 默认监听 6443 端口。

## kube-controller-manager

- 功能：监控、管理 Node、Pod、Service 等各种 k8s 资源。
- 作为单进程运行，但包含多个 controller ，比如：
  - node controller
  - job controller
  - endpoints controller
  - service account controller ：为新建的 namespace 创建 default service account 。

## kube-scheduler

- 功能：决定将 Pod 分配到哪个节点上部署，该过程称为调度。
- 调度分为两个步骤：
  - 过滤
    - ：遍历所有 Node ，筛选出允许调度该 Pod 的所有 Node ，称为可调度节点。比如 Node 必须满足 Pod 的 request 资源、Pod 必须容忍 Node 的污点。
    - 如果没有可调度节点，则 Pod 一直不会部署。
    - 对于节点总数低于 100 的 k8s ，默认设置了 percentageOfNodesToScore=50 ，即当可调度节点数量达到总数的 50% 时就停止遍历，从而减少耗时。
    - 为了避免某些节点一直未被遍历，每次遍历 Node 列表时，会以上一次遍历的终点作为本次遍历的起点。
  - 打分
    - ：给每个可调度节点打分，选出一个最适合部署该 Pod 的 Node 。比如考虑亲和性、污点。
    - 如果存在多个打分最高的 Node ，则随机选取一个。

## kubelet

- 功能：
  - 将当前节点注册到 kube-apiserver 。
  - 监控当前节点。
  - 创建、管理、监控 Pod ，基于容器运行时。
- 默认监听 10250 端口。
- kubelet 部署 Pod 时，会调用 CRI 接口 RuntimeService.RunPodSandbox ，先创建一个沙盒（Pod Sandbox），再启动 Pod 中的容器。
  - Sandbox 负责提供一个 Pod 运行环境，比如设置网络。
  - Sandbox 可以基于 Linux namespace 实现，也可以基于虚拟机实现，比如 kata-containers 。
  - 基于 Linux namespace 实现 Sandbox 时，kubelet 会先在每个 Pod 中运行一个 pause 容器。
    - pause 容器是一个简单程序，便于管理 Linux namespace ，比如创建 network namespace 并共享给其它容器。
    - pause 容器一直以睡眠状态保持运行，避免 Pod 中所有容器进程停止时，Linux namespace 被自动删除。
    - 如果停止 pause 容器，则会导致 kubelet 认为该 Pod 失败，触发重启事件，创建新 Pod 。
    - pause 容器可以与其它容器共用一个 PID namespace ，从而为其它容器启动 1 号进程、清理僵尸进程。不过 k8s 默认禁用了该共享功能，使得其它容器的 1 号进程的 PID 依然为 1 。
- kubelet 中的 PLEG（Pod Lifecycle Event Generator）模块负责执行 relist 任务：获取本机的容器列表，检查所有 Pod 的状态，如果状态变化则生成 Pod 的生命周期事件。
  - 每执行一次 relist ，会等 1s 再执行下一次 list 。
  - 如果某次 relist 耗时超过 3min ，则报错 `PLEG is not healthy` ，并将当前 Node 标记为 NotReady 状态。

## kube-proxy

- 功能：管理节点的网络，将访问 Service 的流量反向代理到 Pod 。

有多种代理模式：
- userspace
  - k8s v1.2 之前的默认模式。
  - 原理：
    - kube-proxy 监听一些端口，反向代理到 Pod 。
    - 配置 iptables 规则，将访问 Service IP 的流量转发到 kube-proxy 监听的端口。
  - 缺点：
    - 流量会先后被 iptables、kube-proxy 转发，需要从内核态传递到用户态，性能较低。
- iptables
  - k8s v1.2 之后的默认模式。
  - 原理：
    - 监听所有 Service、EndPoints 的变化，自动配置 iptables 规则，将访问 Service IP 的流量转发到 EndPoints 。
    - 如果 EndPoints 包含多个 Pod IP ，则有两种负载均衡算法：随机、轮询。
    - 转发数据包时会进行 NAT ，实现透明代理。
      - 将数据包转发给 EndPoints 时，会将数据包的目标 IP 改为 Pod IP ，即 DNAT 。
      - 转发 EndPoints 返回的数据包时，会将数据包的源 IP 改为 Pod IP ，即 SNAT 。
  - 缺点：
    - 修改 iptables 规则时，需要先用 iptables-save 导出，然后修改，最后用 iptables-restore 导入，有一定耗时。
    - 处理每个数据包时，需要线性查找与其匹配的 iptables 规则，时间复杂度为 O(n) 。因此 Service 数量较多时，耗时较久。
- IPVS
  - k8s v1.8 新增的模式，基于 LVS 的 NAT 代理模式。
  - 原理：
    - 为每个 Service IP 创建一个 IPVS ，负责反向代理。
    - 通过 ipset 命令创建一些包含 Service IP 的哈希集合。然后配置 iptables 规则，将访问 ipset 集合的流量交给 IPVS 处理，并进行 NAT 。
  - 优点：
    - 通过 ipset 大幅减少了 iptables 规则的数量，并且哈希查找的速度更快。
    - 支持多种负载均衡算法。

## etcd

- 功能：提供分布式数据库，存储 k8s 的配置、状态数据。
- 默认监听 2379、2380 端口。
  - 一般将 etcd 部署在主节点上，仅供本机的 apiserver 访问。
  - 也可以将 etcd 部署在无 apiserver 的主机上，或者部署在 k8s 集群之外。

